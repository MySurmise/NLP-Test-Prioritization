{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-GPTQ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "import argparse\n",
    "\n",
    "model_name_or_path = \"TheBloke/WizardLM-13B-V1-1-SuperHOT-8K-GPTQ\"\n",
    "model_basename = \"wizardlm-13b-v1.1-superhot-8k-GPTQ-4bit-128g.no-act.order\"\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(\n",
    "    model_name_or_path,\n",
    "    model_basename=model_basename,\n",
    "    use_safetensors=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    use_triton=use_triton,\n",
    "    quantize_config=None,\n",
    ")\n",
    "\n",
    "model.seqlen = 8192\n",
    "\n",
    "# Note: check the prompt template is correct for this model.\n",
    "prompt = \"Tell me about AI\"\n",
    "prompt_template = f\"\"\"USER: {prompt}\n",
    "ASSISTANT:\"\"\"\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors=\"pt\").input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "# Prevent printing spurious transformers error when using pipeline with AutoGPTQ\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15,\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, logging\n",
    "\n",
    "logging.set_verbosity(logging.DEBUG)\n",
    "\n",
    "# Initialize a text-generation pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"HuggingFaceH4/zephyr-7b-beta\")\n",
    "\n",
    "# Generate text based on a prompt\n",
    "prompt = \"In a distant future, humanity has\"\n",
    "generated_text = generator(\n",
    "    prompt, max_length=50, num_return_sequences=5, temperature=0.7\n",
    ")\n",
    "\n",
    "for i, text in enumerate(generated_text):\n",
    "    print(f\"Generated text {i+1}:\")\n",
    "    print(text[\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
